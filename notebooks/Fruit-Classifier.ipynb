{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fruit Classification using Transfer Learning\n",
        "\n",
        "This project implements a deep learning model for fruit image classification using transfer learning with the VGG16 architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Configure the environment by suppressing unnecessary warnings to keep the output clean and focused on important information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.dataset_utils\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.epoch_iterator\")\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow info messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Download and extract the Fruits-360 dataset. The dataset contains fruit images organized into training, validation, and test sets. The extraction process handles large files efficiently by processing them in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract the fruit dataset\n",
        "import os\n",
        "import zipfile\n",
        "from urllib.request import urlretrieve\n",
        "from urllib.error import URLError\n",
        "\n",
        "# Dataset configuration (paths relative to notebook location in notebooks/ directory)\n",
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4yIRGlIpNfKE/fruits-360-original-size.zip\"\n",
        "local_zip = \"../fruits-360-original-size.zip\"\n",
        "extract_dir = \"../fruits-360-original-size\"\n",
        "\n",
        "def download_dataset(url, output_file):\n",
        "    \"\"\"Download dataset from URL using Python's urllib.\"\"\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    print(\"This may take several minutes depending on your internet connection...\")\n",
        "    try:\n",
        "        def reporthook(blocknum, blocksize, totalsize):\n",
        "            \"\"\"Display download progress.\"\"\"\n",
        "            downloaded = blocknum * blocksize\n",
        "            if totalsize > 0:\n",
        "                percent = min(100, (downloaded * 100) / totalsize)\n",
        "                print(f\"\\rProgress: {percent:.1f}% ({downloaded / (1024*1024):.1f} MB)\", end='', flush=True)\n",
        "        \n",
        "        urlretrieve(url, output_file, reporthook=reporthook)\n",
        "        print(\"\\nDownload completed.\")\n",
        "    except URLError as e:\n",
        "        print(f\"\\nError downloading dataset: {e}\")\n",
        "        print(\"Please check your internet connection and try again.\")\n",
        "        raise\n",
        "\n",
        "def extract_zip_in_chunks(zip_file, extract_to, batch_size=2000):\n",
        "    \"\"\"Extract large zip file in chunks to manage memory efficiently.\"\"\"\n",
        "    print(\"Extracting dataset...\")\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        files = zip_ref.namelist()\n",
        "        total_files = len(files)\n",
        "        for i in range(0, total_files, batch_size):\n",
        "            batch = files[i:i+batch_size]\n",
        "            for file in batch:\n",
        "                zip_ref.extract(file, extract_to)\n",
        "            print(f\"Extracted {min(i+batch_size, total_files)} of {total_files} files...\")\n",
        "    print(f\"Dataset extracted to '{extract_to}'.\")\n",
        "\n",
        "# Download and extract dataset if not already present\n",
        "if not os.path.exists(local_zip):\n",
        "    download_dataset(url, local_zip)\n",
        "else:\n",
        "    print(\"Dataset already downloaded.\")\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    extract_zip_in_chunks(local_zip, extract_dir)\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "# Clean up zip file after extraction\n",
        "if os.path.exists(local_zip):\n",
        "    os.remove(local_zip)\n",
        "    print(f\"Cleaned up zip file: {local_zip}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Library Imports and Path Configuration\n",
        "\n",
        "Import all necessary libraries for deep learning, image processing, and visualization. Define the paths to the training, validation, and test directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set dataset paths (relative to notebook location in notebooks/ directory)\n",
        "train_dir = '../fruits-360-original-size/fruits-360-original-size/Training'\n",
        "val_dir = '../fruits-360-original-size/fruits-360-original-size/Validation'\n",
        "test_dir = '../fruits-360-original-size/fruits-360-original-size/Test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generators Setup\n",
        "\n",
        "Create data generators that load images from directories and apply preprocessing. For training data, we apply augmentation techniques (rotation, shifting, zooming, flipping) to increase dataset diversity and improve model generalization. Validation and test sets only require rescaling to normalize pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure data generators with augmentation for training\n",
        "# Training data: apply augmentation to improve generalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation and test data: only rescale (no augmentation)\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {train_generator.samples}\")\n",
        "print(f\"Validation samples: {val_generator.samples}\")\n",
        "print(f\"Test samples: {test_generator.samples}\")\n",
        "print(f\"Number of classes: {train_generator.num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "Build the classification model using transfer learning with VGG16. The pre-trained VGG16 model (trained on ImageNet) serves as the feature extractor. We freeze its layers to preserve the learned weights and add custom classification layers on top. The architecture includes global average pooling, dense layers with batch normalization and dropout for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model using VGG16 as base with transfer learning\n",
        "# Load pre-trained VGG16 model (without top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Freeze base model layers to preserve pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),  # Reduces spatial dimensions\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),  # Stabilizes training\n",
        "    Dropout(0.3),  # Prevents overfitting\n",
        "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Compilation\n",
        "\n",
        "Compile the model with categorical cross-entropy loss (suitable for multi-class classification) and the Adam optimizer. Accuracy is used as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile model with appropriate loss and optimizer\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Training\n",
        "\n",
        "Train the model with frozen base layers. We use callbacks to optimize training:\n",
        "- **ReduceLROnPlateau**: Automatically reduces learning rate when validation loss plateaus\n",
        "- **EarlyStopping**: Stops training if validation loss doesn't improve, preventing overfitting\n",
        "\n",
        "This initial training phase allows the custom classification layers to learn fruit-specific features while leveraging the pre-trained VGG16 feature extractor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=2,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "steps_per_epoch = 50\n",
        "validation_steps = 25\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "Fine-tune the model by unfreezing the last few layers of the VGG16 base model. This allows the model to adapt the pre-trained features specifically for fruit classification. We use a lower learning rate to make gradual adjustments without disrupting the learned features. Batch normalization layers remain frozen to maintain training stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune model by unfreezing last layers of base model\n",
        "# Unfreeze last 5 layers for fine-tuning\n",
        "num_layers = len(base_model.layers)\n",
        "print(f\"Base model has {num_layers} layers\")\n",
        "\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Keep BatchNorm layers frozen for stability\n",
        "for layer in base_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate for fine-tuning\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training with fine-tuning\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluate the trained model on the test set to assess its performance on unseen data. This provides an unbiased estimate of the model's generalization capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=50, verbose=1)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "model.save('../models/fruit_classifier_model.h5')\n",
        "print(\"Model saved to ../models/fruit_classifier_model.h5\")\n",
        "\n",
        "# Save class indices mapping\n",
        "import json\n",
        "class_indices = train_generator.class_indices\n",
        "# Invert the dictionary so numbers map to names: {0: 'Apple', 1: 'Banana'}\n",
        "label_map = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Save it to a file\n",
        "os.makedirs('../config', exist_ok=True)\n",
        "with open('../config/class_indices.json', 'w') as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "print(\"Class indices saved to ../config/class_indices.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Visualization\n",
        "\n",
        "Visualize the training progress by plotting accuracy and loss curves for both the initial training and fine-tuning phases. These plots help identify overfitting, underfitting, and the effectiveness of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Initial Training Accuracy', linestyle='--')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Initial Validation Accuracy', linestyle='--')\n",
        "axes[0].plot([x + len(history.history['accuracy']) for x in range(len(history_fine.history['accuracy']))], \n",
        "             history_fine.history['accuracy'], label='Fine-tuned Training Accuracy')\n",
        "axes[0].plot([x + len(history.history['val_accuracy']) for x in range(len(history_fine.history['val_accuracy']))], \n",
        "             history_fine.history['val_accuracy'], label='Fine-tuned Validation Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot loss\n",
        "axes[1].plot(history.history['loss'], label='Initial Training Loss', linestyle='--')\n",
        "axes[1].plot(history.history['val_loss'], label='Initial Validation Loss', linestyle='--')\n",
        "axes[1].plot([x + len(history.history['loss']) for x in range(len(history_fine.history['loss']))], \n",
        "             history_fine.history['loss'], label='Fine-tuned Training Loss')\n",
        "axes[1].plot([x + len(history.history['val_loss']) for x in range(len(history_fine.history['val_loss']))], \n",
        "             history_fine.history['val_loss'], label='Fine-tuned Validation Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Testing\n",
        "\n",
        "Test the model on sample images from the test set. For each image, we display the true label, predicted label, and prediction confidence. This provides a visual assessment of the model's performance on individual examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predictions on sample images\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from collections import Counter\n",
        "\n",
        "# Get class index mapping\n",
        "class_index_mapping = train_generator.class_indices\n",
        "index_to_class = {v: k for k, v in class_index_mapping.items()}\n",
        "\n",
        "def predict_and_visualize(img_path):\n",
        "    \"\"\"Load image, make prediction, and display results.\"\"\"\n",
        "    # Extract true label from directory structure\n",
        "    true_class = os.path.basename(os.path.dirname(img_path))\n",
        "    \n",
        "    # Load and preprocess image\n",
        "    img = load_img(img_path, target_size=(64, 64))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(img_array, verbose=0)\n",
        "    predicted_index = np.argmax(prediction, axis=-1)[0]\n",
        "    predicted_class = index_to_class[predicted_index]\n",
        "    confidence = prediction[0][predicted_index]\n",
        "    \n",
        "    # Display results\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True: {true_class}\\nPredicted: {predicted_class}\\nConfidence: {confidence:.2%}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    return true_class, predicted_class, confidence\n",
        "\n",
        "# Test on sample images (paths relative to notebook location)\n",
        "sample_images = [\n",
        "    '../fruits-360-original-size/fruits-360-original-size/Test/apple_braeburn_1/r0_11.jpg',\n",
        "    '../fruits-360-original-size/fruits-360-original-size/Test/pear_1/r0_103.jpg',\n",
        "    '../fruits-360-original-size/fruits-360-original-size/Test/cucumber_3/r0_103.jpg'\n",
        "]\n",
        "\n",
        "print(\"Testing model predictions on sample images:\\n\")\n",
        "for img_path in sample_images:\n",
        "    if os.path.exists(img_path):\n",
        "        predict_and_visualize(img_path)\n",
        "    else:\n",
        "        print(f\"Image not found: {img_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
